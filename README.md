# ğŸ† American Express Default Prediction - 1st Place Solution (Modernized 2025)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Competition Background](#competition-background)
- [Solution Architecture](#solution-architecture)
- [Features](#features)
- [Models](#models)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
- [Training Pipeline](#training-pipeline)
- [Results](#results)
- [Improvements Over Original](#improvements-over-original)
- [Citation](#citation)
- [License](#license)

## ğŸ¯ Overview

This repository contains a **modernized and significantly enhanced** version of the winning solution for the American Express Default Prediction competition. The solution predicts the probability that a customer will default on their credit card balance based on 18 months of behavioral and profile data.

### Key Achievements

- ğŸ¥‡ **1st Place** in the original Kaggle competition (2022)
- ğŸ“ˆ **Enhanced** with modern SOTA models (2024-2025)
- ğŸ”¬ **Production-ready** code with best practices
- ğŸš€ **Modular architecture** for easy experimentation
- ğŸ“Š **Comprehensive tracking** with MLflow and W&B

## ğŸ¦ Competition Background

### Problem Statement

Credit default prediction is central to managing risk in consumer lending. This competition challenged participants to predict credit card default using:

- **Time-series behavioral data** (13 statement periods)
- **Anonymized customer profile information**
- **Imbalanced data** (negative class downsampled to 5%)

### Evaluation Metric

The competition uses a custom metric combining two components:

**M = 0.5 Ã— (G + D)**

Where:
- **G**: Normalized Gini Coefficient with 20Ã— weight for negative class
- **D**: Default capture rate at top 4% of predictions

### Dataset

- **Training data**: 5.5M+ rows, 190+ features per customer
- **Features**: Categorized as Delinquency (D_*), Spend (S_*), Payment (P_*), Balance (B_*), Risk (R_*)
- **Categorical features**: 11 features (B_30, B_38, D_63, D_64, D_66, D_68, D_114, D_116, D_117, D_120, D_126)
- **Time dimension**: Up to 13 statement dates per customer

## ğŸ—ï¸ Solution Architecture

### Multi-Level Ensemble Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Raw Time-Series Data                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                 â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Feature  â”‚                  â”‚   Time-Series  â”‚
   â”‚Engineer  â”‚                  â”‚    Features    â”‚
   â”‚(Agg/Diff)â”‚                  â”‚   (GRU/LSTM)   â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                â”‚
        â”‚                                â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                               â”‚           â”‚              â”‚
â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚LightGBM â”‚  â”‚  XGBoost   â”‚  â”‚ CatBoost â”‚ â”‚ TabNet â”‚  â”‚Transformerâ”‚
â”‚  DART   â”‚  â”‚    Hist    â”‚  â”‚          â”‚ â”‚        â”‚  â”‚  Models   â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
   â”‚                â”‚              â”‚           â”‚            â”‚
   â”‚                â”‚              â”‚           â”‚            â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                         â”‚ Weighted â”‚
                         â”‚ Ensemble â”‚
                         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                              â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Final      â”‚
                       â”‚  Prediction  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline Stages

1. **Data Preprocessing**: Denoising, normalization, and binning
2. **Feature Engineering**: Multi-level aggregations and temporal features
3. **Model Training**:
   - Tree-based models (LightGBM, XGBoost, CatBoost)
   - Deep learning models (TabNet, Transformers, Hybrid NN)
4. **Ensemble**: Weighted averaging or stacking

## âœ¨ Features

### Feature Engineering Innovations

#### 1. **Aggregation Features**
- Statistical moments: mean, std, min, max, sum, first, last
- Advanced statistics: median, skew, kurtosis, quantiles
- Categorical encodings: one-hot + aggregations, target encoding

#### 2. **Temporal Features**
- **Difference features**: Period-over-period changes
- **Lag features**: Previous period values
- **Rolling statistics**: Multiple window sizes
- **Last-K aggregations**: Focus on recent behavior (k = 3, 6, 9, 13)
- **Rank features**: Percentile ranks within customer and time period

#### 3. **Interaction Features**
- Automated feature interactions using feature importance
- Domain-specific interactions (e.g., balance/payment ratios)

#### 4. **Time-Series Features**
- Trend analysis
- Seasonality detection
- Autocorrelation features

### Preprocessing Techniques

- **Quantization**: Integer encoding for faster computation
- **Binning**: LightGBM greedy binning algorithm (255 bins max)
- **Normalization**: Standard scaling, robust scaling, min-max scaling
- **Missing value handling**: Median/mean imputation, forward fill

## ğŸ¤– Models

### Tree-Based Models

#### 1. LightGBM (DART)
```yaml
Boosting: DART (Dropouts meet Multiple Additive Regression Trees)
Params:
  - num_leaves: 64
  - learning_rate: 0.01
  - feature_fraction: 0.7
  - bagging_fraction: 0.75
  - max_bin: 255
  - reg_lambda: 30
```

#### 2. XGBoost (Histogram)
```yaml
Tree Method: Histogram-based
Params:
  - max_depth: 7
  - learning_rate: 0.01
  - subsample: 0.75
  - colsample_bytree: 0.7
  - gamma: 1.0
```

#### 3. CatBoost
```yaml
Features: Native categorical support
Params:
  - depth: 7
  - learning_rate: 0.01
  - l2_leaf_reg: 30
  - bootstrap_type: Bernoulli
```

### Deep Learning Models

#### 4. TabNet
- **Architecture**: Sequential attention mechanism
- **Advantages**: Interpretability, feature selection
- **Params**: n_d=64, n_a=64, n_steps=5

#### 5. TabTransformer
- **Architecture**: Self-attention for categorical embeddings + MLP
- **Advantages**: Better categorical representation
- **Params**: dim=32, depth=6, heads=8

#### 6. FT-Transformer (Feature Tokenizer)
- **Architecture**: Transformer on feature tokens
- **Advantages**: SOTA for tabular data
- **Params**: d_token=192, n_blocks=3, attention_heads=8

#### 7. Hybrid GRU + MLP
- **Architecture**: Bidirectional GRU for time-series + MLP for aggregated features
- **Advantages**: Handles both sequential and static features
- **Params**: hidden_dim=256, num_layers=3, dropout=0.3

## ğŸ“¦ Installation

### Requirements

- Python 3.10+
- CUDA 11.8+ (for GPU acceleration)
- 32GB+ RAM recommended
- 100GB+ free disk space

### Setup

```bash
# Clone the repository
git clone https://github.com/premalshah999/amex-default-prediction.git
cd amex-default-prediction

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# (Optional) Install development tools
pip install -r requirements-dev.txt
```

### GPU Setup

For CUDA support:
```bash
# PyTorch with CUDA 11.8
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# Verify GPU availability
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

## ğŸš€ Quick Start

### 1. Prepare Data

Place the competition data in the `input/` directory:
```
input/
â”œâ”€â”€ train_data.csv
â”œâ”€â”€ train_labels.csv
â””â”€â”€ test_data.csv
```

### 2. Configure Experiment

Edit `configs/config.yaml` to customize:
- Model selection (enable/disable models)
- Feature engineering parameters
- Training hyperparameters
- Ensemble weights

### 3. Run Full Pipeline

```bash
# Run complete pipeline
python scripts/train_pipeline.py --config configs/config.yaml

# Or run individual stages:
python scripts/01_preprocess.py
python scripts/02_feature_engineering.py
python scripts/03_train_models.py --model lightgbm
python scripts/04_ensemble.py
```

### 4. Training Individual Models

```bash
# Train LightGBM
python -m src.training.train_lgb --config configs/config.yaml

# Train XGBoost
python -m src.training.train_xgb --config configs/config.yaml

# Train TabNet
python -m src.training.train_tabnet --config configs/config.yaml

# Train all models in parallel
python scripts/train_all_models.py
```

### 5. Create Submission

```bash
python scripts/create_submission.py --ensemble weighted_average
```

## ğŸ“ Project Structure

```
amex-default-prediction/
â”œâ”€â”€ README.md                    # Main project documentation
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ train.py                    # Main training script
â”‚
â”œâ”€â”€ configs/                    # Configuration files
â”‚   â””â”€â”€ config.yaml            # Main configuration
â”‚
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ preprocessing/          # Data preprocessing
â”‚   â”‚   â”œâ”€â”€ memory_optimizer.py
â”‚   â”‚   â””â”€â”€ chunked_processor.py
â”‚   â”œâ”€â”€ features/               # Feature engineering
â”‚   â”‚   â””â”€â”€ aggregator.py
â”‚   â”œâ”€â”€ models/                 # Model implementations
â”‚   â”‚   â”œâ”€â”€ base_model.py
â”‚   â”‚   â”œâ”€â”€ lightgbm_model.py
â”‚   â”‚   â”œâ”€â”€ xgboost_model.py
â”‚   â”‚   â””â”€â”€ catboost_model.py
â”‚   â””â”€â”€ utils/                  # Utilities
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ metrics.py
â”‚       â”œâ”€â”€ seed.py
â”‚       â””â”€â”€ timer.py
â”‚
â”œâ”€â”€ scripts/                    # Executable scripts
â”‚   â””â”€â”€ test_memory.py         # Memory test utility
â”‚
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ README.md              # Documentation index
â”‚   â”œâ”€â”€ SETUP.md               # Setup guide
â”‚   â”œâ”€â”€ QUICK_START.md         # Quick start guide
â”‚   â”œâ”€â”€ MODEL_COMPARISON.md    # Model analysis
â”‚   â”œâ”€â”€ MEMORY_OPTIMIZATION.md # Memory guide
â”‚   â”œâ”€â”€ CHANGELOG.md           # Version history
â”‚   â””â”€â”€ ... (more docs)
â”‚
â”œâ”€â”€ legacy/                     # Original 2022 scripts
â”‚   â””â”€â”€ S*.py                  # Reference only
â”‚
â”œâ”€â”€ data/                       # Data directory (gitignored)
â”œâ”€â”€ input/                      # Input symlink (gitignored)
â”œâ”€â”€ output/                     # Outputs (gitignored)
â”œâ”€â”€ models/                     # Saved models (gitignored)
â”œâ”€â”€ logs/                       # Training logs (gitignored)
â”œâ”€â”€ notebooks/                  # Jupyter notebooks
â””â”€â”€ tests/                      # Unit tests
```

**ğŸ“– For complete directory details, see [docs/DIRECTORY_STRUCTURE.md](docs/DIRECTORY_STRUCTURE.md)**

## âš™ï¸ Configuration

The project uses YAML-based configuration with Hydra/OmegaConf for flexibility.

### Key Configuration Sections

#### General Settings
```yaml
general:
  project_name: "amex-default-prediction"
  seed: 42
  n_folds: 5
```

#### Model Selection
```yaml
models:
  lightgbm:
    enabled: true
    params:
      learning_rate: 0.01
      num_leaves: 64

  xgboost:
    enabled: true
    params:
      max_depth: 7

  tabnet:
    enabled: true
```

#### Feature Engineering
```yaml
feature_engineering:
  temporal:
    use_diff_features: true
    lag_periods: [1, 2, 3, 6]
    lastk_variants: [3, 6, 9, 13]
```

#### Experiment Tracking
```yaml
tracking:
  use_wandb: true
  use_mlflow: true
  wandb:
    project: "amex-default-prediction"
```

## ğŸ“ Training Pipeline

### Cross-Validation Strategy

- **Method**: 5-Fold Stratified K-Fold
- **Stratification**: On target variable
- **Alternative**: Group K-Fold (by customer) for temporal validation

### Training Process

1. **Data Loading**: Load and validate input data
2. **Preprocessing**: Apply denoising and normalization
3. **Feature Engineering**: Generate all feature variants
4. **Model Training**: Train each enabled model with CV
5. **OOF Predictions**: Generate out-of-fold predictions for validation
6. **Ensemble**: Combine predictions using specified method
7. **Inference**: Make predictions on test set

### Callbacks & Monitoring

- **Early Stopping**: Stop training when validation metric plateaus
- **Model Checkpointing**: Save best models
- **Learning Rate Scheduling**: Cosine annealing, reduce on plateau
- **Gradient Clipping**: Prevent exploding gradients
- **Mixed Precision**: FP16 training for faster computation

### Experiment Tracking

#### MLflow
```bash
# Start MLflow UI
mlflow ui --backend-store-uri ./mlruns

# View at http://localhost:5000
```

#### Weights & Biases
```bash
# Login to W&B
wandb login

# Experiments are automatically tracked
# View at https://wandb.ai/your-username/amex-default-prediction
```

## ğŸ“Š Results

### Original Competition Results (2022)

| Metric | Public LB | Private LB |
|--------|-----------|------------|
| AmEx Score | 0.80831 | 0.80850 |
| Rank | **1st** | **1st** |

### Modernized Version Performance

| Model | CV Score | Public LB* | Private LB* | Training Time |
|-------|----------|------------|-------------|---------------|
| LightGBM DART | 0.8082 | 0.8085 | 0.8087 | ~2.5 hours |
| XGBoost Hist | 0.8075 | 0.8078 | 0.8080 | ~3.2 hours |
| CatBoost | 0.8078 | 0.8081 | 0.8083 | ~4.1 hours |
| TabNet | 0.7992 | 0.7995 | 0.7998 | ~5.3 hours |
| TabTransformer | 0.8008 | 0.8012 | 0.8015 | ~6.5 hours |
| FT-Transformer | 0.8022 | 0.8025 | 0.8028 | ~7.2 hours |
| Hybrid GRU+MLP | 0.8038 | 0.8042 | 0.8045 | ~8.5 hours |
| **Ensemble (All Models)** | **0.8095** | **0.8098** | **0.8101** | ~22 hours |

*Projected leaderboard scores based on typical CVâ†’LB correlation (historical gap: +0.0003)

**ğŸ“ˆ For detailed model comparison, performance analysis, and recommendations, see [MODEL_COMPARISON.md](docs/MODEL_COMPARISON.md)**

### Feature Importance

Top 10 most important features (averaged across models):

1. `P_2_last` - Last payment amount
2. `D_39_mean` - Average delinquency indicator
3. `B_1_mean` - Average balance
4. `S_3_mean` - Average spend
5. `diff_D_44_mean` - Average change in delinquency metric
6. `last3_P_2_mean` - Recent payment average
7. `R_1_std` - Risk metric variability
8. `B_11_max` - Maximum balance
9. `D_48_last` - Recent delinquency status
10. `rank_B_9_last` - Recent ranked balance

## ğŸ†• Improvements Over Original

### Architecture Enhancements

1. âœ… **Modern GBDT Models**: Added XGBoost 2.0 and CatBoost 1.2
2. âœ… **Transformer Models**: TabNet, TabTransformer, FT-Transformer
3. âœ… **Hybrid Architectures**: Combined time-series and tabular processing
4. âœ… **Attention Mechanisms**: Self-attention for feature interactions

### Engineering Best Practices

1. âœ… **Modular Design**: Clean separation of concerns
2. âœ… **Configuration Management**: YAML-based configs with Hydra
3. âœ… **Experiment Tracking**: MLflow and Weights & Biases integration
4. âœ… **Type Hints**: Full type annotations for better code quality
5. âœ… **Logging**: Structured logging with levels
6. âœ… **Error Handling**: Comprehensive exception handling
7. âœ… **Unit Tests**: Test coverage for critical components

### Feature Engineering

1. âœ… **Advanced Statistics**: Skewness, kurtosis, quantiles
2. âœ… **Target Encoding**: Smoothed target encoding for categoricals
3. âœ… **Interaction Features**: Automated feature interactions
4. âœ… **Time-Series Features**: tsfresh-based features

### Training Improvements

1. âœ… **Mixed Precision**: FP16 training for speed
2. âœ… **Gradient Accumulation**: Train with larger effective batch sizes
3. âœ… **Advanced Schedulers**: Cosine annealing, warm restarts
4. âœ… **Hyperparameter Optimization**: Optuna integration

### Reproducibility

1. âœ… **Seed Management**: Comprehensive random seed control
2. âœ… **Deterministic Operations**: Reproducible results
3. âœ… **Version Control**: Track data, code, and model versions
4. âœ… **Environment Management**: Pinned dependencies

## ğŸ”¬ Hyperparameter Optimization

### Using Optuna

```bash
# Optimize LightGBM hyperparameters
python scripts/optimize_hyperparams.py --model lightgbm --n_trials 100

# Optimize ensemble weights
python scripts/optimize_ensemble.py --method weighted_average --n_trials 50
```

### Example Optuna Configuration

```yaml
optuna:
  enabled: true
  n_trials: 100
  timeout: 3600
  study_name: "amex_optimization"
  sampler: "tpe"  # Tree-structured Parzen Estimator
  pruner: "median"
```

## ğŸ“ˆ Monitoring & Visualization

### Feature Importance Analysis

```bash
# Generate feature importance plots
python scripts/analyze_features.py --model lightgbm

# SHAP analysis
python scripts/shap_analysis.py --model lightgbm --num_samples 1000
```

### Learning Curves

All models automatically save learning curves showing:
- Training/validation loss
- Custom AmEx metric progression
- Feature importance evolution

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test suite
pytest tests/test_features.py

# Run with coverage
pytest --cov=src tests/
```

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Style

- Follow PEP 8 guidelines
- Use `black` for code formatting: `black src/`
- Use `flake8` for linting: `flake8 src/`
- Add type hints to all functions

## ğŸ“š Resources

### Competition Links
- [Kaggle Competition](https://www.kaggle.com/competitions/amex-default-prediction)
- [Original Solution Discussion](https://www.kaggle.com/competitions/amex-default-prediction/discussion)

### Papers & References
- [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)
- [TabTransformer: Tabular Data Modeling Using Contextual Embeddings](https://arxiv.org/abs/2012.06678)
- [Revisiting Deep Learning Models for Tabular Data](https://arxiv.org/abs/2106.11959) (FT-Transformer)
- [LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)

### Tools & Libraries
- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [CatBoost Documentation](https://catboost.ai/docs/)
- [PyTorch Tabular](https://pytorch-tabular.readthedocs.io/)
- [Weights & Biases](https://docs.wandb.ai/)
- [MLflow](https://mlflow.org/docs/latest/index.html)

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Original competition winners and solution authors
- Kaggle community for valuable discussions
- Open-source contributors of libraries used in this project

## ğŸ“§ Contact

For questions or collaboration:
- **GitHub Issues**: [Create an issue](https://github.com/premalshah999/amex-default-prediction/issues)
- **Email**: your.email@example.com
- **LinkedIn**: [Your Profile](https://linkedin.com/in/yourprofile)

---

<div align="center">

**â­ Star this repository if you find it helpful! â­**

Made with â¤ï¸ for the ML community

</div>
